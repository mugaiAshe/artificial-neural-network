{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "from tensorboardX import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （一）数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.txt'\n",
    "dev_path = './data/dev.txt'\n",
    "\n",
    "i = 0\n",
    "label_dict = {}\n",
    "\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "for line in train_lines:\n",
    "    label = line.split('\\t')[1].replace('\\n', '')\n",
    "    if label not in label_dict:\n",
    "        label_dict[label] = i\n",
    "        i += 1\n",
    "\n",
    "with open('data/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_lines[0:int(len(train_lines)*0.8)]:\n",
    "        title = line.split('\\t')[0]\n",
    "        label = line.split('\\t')[1].replace('\\n', '')\n",
    "        f.write(title + '\\t' + str(label_dict[label]) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('data/test.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_lines[int(len(train_lines)*0.8):]:\n",
    "        title = line.split('\\t')[0]\n",
    "        label = line.split('\\t')[1].replace('\\n', '')\n",
    "        f.write(title + '\\t' + str(label_dict[label]) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open(dev_path, 'r', encoding='utf-8') as f:\n",
    "    dev_lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "with open('data/dev.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in dev_lines:\n",
    "        title = line.split('\\t')[0]\n",
    "        label = line.split('\\t')[1].replace('\\n', '')\n",
    "        f.write(title + '\\t' + str(label_dict[label]) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "label_list = list(label_dict)\n",
    "with open('data/dict.txt', 'w', encoding='utf-8') as f:\n",
    "    for label in label_list:\n",
    "        f.write(label + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000  # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = lambda x: [y for y in x]  # char-level\n",
    "\n",
    "def build_vocab(file_path, tokenizer, max_size, min_freq):\n",
    "    vocab_dic = {}\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            content = lin.split('\\t')[0]\n",
    "            for word in tokenizer(content):\n",
    "                vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "        vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n",
    "        vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "        vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n",
    "    return vocab_dic\n",
    "\n",
    "if os.path.exists('data/vocab.pkl'):\n",
    "    vocab = pkl.load(open('data/vocab.pkl', 'rb'))\n",
    "else:\n",
    "    vocab = build_vocab('data/train.txt', tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "    pkl.dump(vocab, open('data/vocab.pkl', 'wb'))\n",
    "class_list = [x.strip() for x in open('data/dict.txt', encoding='utf-8').readlines()]\n",
    "num_class = len(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （二）数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "385264it [00:04, 91452.19it/s] \n",
      "80000it [00:00, 91877.24it/s] \n",
      "96316it [00:01, 92855.92it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def load_dataset(path, pad_size=32):\n",
    "    contents = []\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            content, label = lin.split('\\t')\n",
    "            words_line = []\n",
    "            token = tokenizer(content)\n",
    "            seq_len = len(token)\n",
    "            if pad_size:\n",
    "                if len(token) < pad_size:\n",
    "                    token.extend([PAD] * (pad_size - len(token)))\n",
    "                else:\n",
    "                    token = token[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            # word to id\n",
    "            for word in token:\n",
    "                words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "            contents.append((words_line, int(label), seq_len))\n",
    "    return contents  # [([...], 0), ([...], 1), ...]\n",
    "\n",
    "\n",
    "train_data = load_dataset('data/train.txt', 32)\n",
    "dev_data = load_dataset('data/dev.txt', 32)\n",
    "test_data = load_dataset('data/test.txt', 32)\n",
    "\n",
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False  # 记录batch数量是否为整数\n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "\n",
    "        # pad前的长度(超过pad_size的设为pad_size)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches\n",
    "\n",
    "\n",
    "train_iter = DatasetIterater(train_data, 128, device)\n",
    "dev_iter = DatasetIterater(dev_data, 128, device)\n",
    "test_iter = DatasetIterater(test_data, 128, device)\n",
    "\n",
    "num_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （三）定义GAN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_vocab, 300, padding_idx=num_vocab - 1)\n",
    "        self.conv1 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(9600, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(isinstance(x, tuple)):\n",
    "            x = self.embedding(x[0])\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(100, 32 * 300)  # Assuming input noise size is 100\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 32, 300)\n",
    "        return x\n",
    "\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （四）模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, generator, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    discriminator.train()\n",
    "    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir='log/GAN' + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(5):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, 5))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = discriminator(trains)\n",
    "            discriminator.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            random_noise = torch.randn(128, 100).to(device)  # Batch size of 128, noise size of 100\n",
    "            fake_data = generator(random_noise)\n",
    "\n",
    "            # 假图像的标签（你可以根据需要定义）\n",
    "            fake_labels = torch.randint(0, num_class, (128,)).to(device)\n",
    "\n",
    "            # 鉴别器对假图像的输出\n",
    "            fake_outputs = discriminator(fake_data.detach())\n",
    "            fake_loss = F.cross_entropy(fake_outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(discriminator, generator, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(discriminator.state_dict(), 'saved_dict/GAN.ckpt')\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                discriminator.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > 1000:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "    model_test(discriminator, generator, test_iter)\n",
    "\n",
    "\n",
    "def model_test(discriminator, generator, test_iter):\n",
    "    # test\n",
    "    discriminator.load_state_dict(torch.load('saved_dict/GAN.ckpt'))\n",
    "    discriminator.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(discriminator, generator, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "def evaluate(discriminator, generator, data_iter, test=False):\n",
    "    discriminator.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = discriminator(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Iter:      0,  Train Loss:   2.6,  Train Acc:  7.81%,  Val Loss:   2.4,  Val Acc: 19.58%,  Time: 0:00:04 *\n",
      "Iter:    100,  Train Loss:  0.76,  Train Acc: 78.12%,  Val Loss:   1.1,  Val Acc: 65.50%,  Time: 0:00:07 *\n",
      "Iter:    200,  Train Loss:  0.54,  Train Acc: 83.59%,  Val Loss:  0.72,  Val Acc: 77.01%,  Time: 0:00:10 *\n",
      "Iter:    300,  Train Loss:  0.64,  Train Acc: 78.12%,  Val Loss:  0.65,  Val Acc: 79.77%,  Time: 0:00:12 *\n",
      "Iter:    400,  Train Loss:  0.53,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 81.93%,  Time: 0:00:15 *\n",
      "Iter:    500,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 82.36%,  Time: 0:00:18 *\n",
      "Iter:    600,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.58,  Val Acc: 81.44%,  Time: 0:00:21 \n",
      "Iter:    700,  Train Loss:  0.55,  Train Acc: 80.47%,  Val Loss:  0.51,  Val Acc: 83.79%,  Time: 0:00:23 *\n",
      "Iter:    800,  Train Loss:  0.38,  Train Acc: 85.16%,  Val Loss:  0.51,  Val Acc: 83.85%,  Time: 0:00:26 *\n",
      "Iter:    900,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.49,  Val Acc: 84.01%,  Time: 0:00:29 *\n",
      "Iter:   1000,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 84.62%,  Time: 0:00:32 *\n",
      "Iter:   1100,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 85.39%,  Time: 0:00:35 *\n",
      "Iter:   1200,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 85.91%,  Time: 0:00:38 *\n",
      "Iter:   1300,  Train Loss:  0.61,  Train Acc: 82.03%,  Val Loss:  0.46,  Val Acc: 85.50%,  Time: 0:00:40 \n",
      "Iter:   1400,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.43,  Val Acc: 86.06%,  Time: 0:00:43 *\n",
      "Iter:   1500,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.43,  Val Acc: 86.26%,  Time: 0:00:46 *\n",
      "Iter:   1600,  Train Loss:  0.26,  Train Acc: 94.53%,  Val Loss:  0.42,  Val Acc: 86.50%,  Time: 0:00:49 *\n",
      "Iter:   1700,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.42,  Val Acc: 86.52%,  Time: 0:00:52 \n",
      "Iter:   1800,  Train Loss:  0.48,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 86.72%,  Time: 0:00:55 *\n",
      "Iter:   1900,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 86.88%,  Time: 0:00:57 \n",
      "Iter:   2000,  Train Loss:  0.35,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 87.81%,  Time: 0:01:00 *\n",
      "Iter:   2100,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 87.20%,  Time: 0:01:03 \n",
      "Iter:   2200,  Train Loss:  0.58,  Train Acc: 82.03%,  Val Loss:  0.37,  Val Acc: 88.12%,  Time: 0:01:06 *\n",
      "Iter:   2300,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.43,  Val Acc: 86.30%,  Time: 0:01:09 \n",
      "Iter:   2400,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.42,  Val Acc: 86.67%,  Time: 0:01:12 \n",
      "Iter:   2500,  Train Loss:  0.35,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.28%,  Time: 0:01:15 *\n",
      "Iter:   2600,  Train Loss:  0.45,  Train Acc: 82.03%,  Val Loss:  0.39,  Val Acc: 87.65%,  Time: 0:01:18 \n",
      "Iter:   2700,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 87.30%,  Time: 0:01:21 \n",
      "Iter:   2800,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 87.93%,  Time: 0:01:24 \n",
      "Iter:   2900,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 88.22%,  Time: 0:01:26 *\n",
      "Iter:   3000,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:   0.4,  Val Acc: 87.16%,  Time: 0:01:29 \n",
      "Epoch [2/5]\n",
      "Iter:   3100,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 88.91%,  Time: 0:01:32 *\n",
      "Iter:   3200,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.38,  Val Acc: 87.87%,  Time: 0:01:35 \n",
      "Iter:   3300,  Train Loss:  0.35,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.25%,  Time: 0:01:38 \n",
      "Iter:   3400,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.37,  Val Acc: 88.11%,  Time: 0:01:40 \n",
      "Iter:   3500,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 88.41%,  Time: 0:01:43 \n",
      "Iter:   3600,  Train Loss:  0.22,  Train Acc: 94.53%,  Val Loss:  0.35,  Val Acc: 88.72%,  Time: 0:01:46 \n",
      "Iter:   3700,  Train Loss:  0.18,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.47%,  Time: 0:01:49 *\n",
      "Iter:   3800,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 88.47%,  Time: 0:01:52 \n",
      "Iter:   3900,  Train Loss:  0.42,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 88.66%,  Time: 0:01:54 \n",
      "Iter:   4000,  Train Loss:  0.25,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 88.84%,  Time: 0:01:57 \n",
      "Iter:   4100,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 88.70%,  Time: 0:02:00 \n",
      "Iter:   4200,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.02%,  Time: 0:02:03 \n",
      "Iter:   4300,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 88.93%,  Time: 0:02:06 \n",
      "Iter:   4400,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 88.67%,  Time: 0:02:09 \n",
      "Iter:   4500,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 88.78%,  Time: 0:02:12 \n",
      "Iter:   4600,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.20%,  Time: 0:02:15 \n",
      "Iter:   4700,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.42%,  Time: 0:02:18 *\n",
      "Iter:   4800,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.15%,  Time: 0:02:20 \n",
      "Iter:   4900,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.51%,  Time: 0:02:23 \n",
      "Iter:   5000,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 88.39%,  Time: 0:02:26 \n",
      "Iter:   5100,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 88.77%,  Time: 0:02:29 \n",
      "Iter:   5200,  Train Loss:  0.13,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 88.49%,  Time: 0:02:32 \n",
      "Iter:   5300,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.51%,  Time: 0:02:34 \n",
      "Iter:   5400,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 88.99%,  Time: 0:02:37 \n",
      "Iter:   5500,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.32,  Val Acc: 89.74%,  Time: 0:02:40 *\n",
      "Iter:   5600,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.57%,  Time: 0:02:43 \n",
      "Iter:   5700,  Train Loss:  0.19,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.22%,  Time: 0:02:46 \n",
      "Iter:   5800,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 89.65%,  Time: 0:02:49 *\n",
      "Iter:   5900,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.34,  Val Acc: 89.28%,  Time: 0:02:52 \n",
      "Iter:   6000,  Train Loss:  0.22,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 88.59%,  Time: 0:02:55 \n",
      "Epoch [3/5]\n",
      "Iter:   6100,  Train Loss:  0.28,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 88.59%,  Time: 0:02:58 \n",
      "Iter:   6200,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 89.69%,  Time: 0:03:01 *\n",
      "Iter:   6300,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.57%,  Time: 0:03:03 \n",
      "Iter:   6400,  Train Loss:  0.21,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.46%,  Time: 0:03:06 \n",
      "Iter:   6500,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.58%,  Time: 0:03:09 \n",
      "Iter:   6600,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.39,  Val Acc: 87.75%,  Time: 0:03:12 \n",
      "Iter:   6700,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 88.75%,  Time: 0:03:15 \n",
      "Iter:   6800,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 89.90%,  Time: 0:03:18 \n",
      "Iter:   6900,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.31,  Val Acc: 90.29%,  Time: 0:03:21 *\n",
      "Iter:   7000,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.82%,  Time: 0:03:23 \n",
      "Iter:   7100,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.75%,  Time: 0:03:26 \n",
      "Iter:   7200,  Train Loss:  0.23,  Train Acc: 95.31%,  Val Loss:  0.32,  Val Acc: 89.88%,  Time: 0:03:29 \n",
      "Iter:   7300,  Train Loss:  0.27,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.23%,  Time: 0:03:32 \n",
      "Iter:   7400,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 88.85%,  Time: 0:03:35 \n",
      "Iter:   7500,  Train Loss:   0.2,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.44%,  Time: 0:03:38 \n",
      "Iter:   7600,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 89.14%,  Time: 0:03:40 \n",
      "Iter:   7700,  Train Loss:  0.19,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.64%,  Time: 0:03:43 \n",
      "Iter:   7800,  Train Loss:  0.11,  Train Acc: 96.09%,  Val Loss:  0.34,  Val Acc: 89.33%,  Time: 0:03:46 \n",
      "Iter:   7900,  Train Loss:  0.13,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 88.84%,  Time: 0:03:49 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.39,  Test Acc: 88.75%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8631    0.9006    0.8815     18810\n",
      "           1     0.9252    0.9904    0.9567     15062\n",
      "           2     0.8891    0.8315    0.8593      7293\n",
      "           3     0.8794    0.8800    0.8797     17673\n",
      "           4     0.9108    0.8934    0.9020     10581\n",
      "           5     0.9294    0.9273    0.9284      4900\n",
      "           6     0.8276    0.8256    0.8266      3756\n",
      "           7     0.8188    0.8198    0.8193      4350\n",
      "           8     0.8650    0.7971    0.8297      2356\n",
      "           9     0.9153    0.8342    0.8729      5934\n",
      "          10     0.8780    0.7756    0.8236      2785\n",
      "          11     0.9669    0.7950    0.8726       844\n",
      "          12     0.9588    0.9041    0.9307       386\n",
      "          13     0.8388    0.8430    0.8409      1586\n",
      "\n",
      "    accuracy                         0.8875     96316\n",
      "   macro avg     0.8905    0.8584    0.8731     96316\n",
      "weighted avg     0.8876    0.8875    0.8869     96316\n",
      "\n",
      "Confusion Matrix...\n",
      "[[16940   105   126   781   193    72   155    90    28   105   186     0\n",
      "      0    29]\n",
      " [   25 14918    19    13    58     8     3     6     0     2     0     7\n",
      "      0     3]\n",
      " [  272   117  6064   383   123    53    36    42    27   144    13     0\n",
      "      1    18]\n",
      " [  920    97   243 15552    57    20    97   557    75    21    22     4\n",
      "      0     8]\n",
      " [  207   489    47    60  9453    42    87    12    12    34    41     0\n",
      "      4    93]\n",
      " [   94    35    38    36    52  4544    12     6     7    56     5     0\n",
      "      6     9]\n",
      " [  190    42    23   124   102    11  3101    20    71    13    14     2\n",
      "      2    41]\n",
      " [  121    30    38   494    20     8    30  3566    21    17     3     1\n",
      "      0     1]\n",
      " [   97    32    29   121    27     9    96    20  1878    38     4     0\n",
      "      0     5]\n",
      " [  283    67   171    54   159   103    36    19    44  4950     8     9\n",
      "      0    31]\n",
      " [  433    12    13    43    52     6    26    12     8    10  2160     0\n",
      "      0    10]\n",
      " [    5   153     1     1     1     0     2     0     0     9     0   671\n",
      "      0     1]\n",
      " [    2     1     0     3     2     3    17     0     0     1     0     0\n",
      "    349     8]\n",
      " [   37    26     8    20    80    10    49     5     0     8     4     0\n",
      "      2  1337]]\n",
      "Time usage: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "train(discriminator, generator, train_iter, dev_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
