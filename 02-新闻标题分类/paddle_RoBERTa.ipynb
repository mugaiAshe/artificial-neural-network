{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:36:56.608080Z",
     "iopub.status.busy": "2024-01-13T16:36:56.607310Z",
     "iopub.status.idle": "2024-01-13T16:36:59.369612Z",
     "shell.execute_reply": "2024-01-13T16:36:59.368454Z",
     "shell.execute_reply.started": "2024-01-13T16:36:56.608028Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_table('./data/train.txt', sep='\\t',header=None)  # 训练集\n",
    "dev = pd.read_table('./data/dev.txt', sep='\\t',header=None)      # 验证集\n",
    "\n",
    "train.columns = [\"text_a\",'label']\n",
    "dev.columns = [\"text_a\",'label']\n",
    "\n",
    "total = pd.concat([train,dev],axis=0)\n",
    "\n",
    "total['label'].value_counts()\n",
    "\n",
    "total['text_a'].map(len).describe()\n",
    "\n",
    "train.to_csv('train.csv', sep='\\t', index=False)  # 保存训练集，格式为text_a,label\n",
    "dev.to_csv('dev.csv', sep='\\t', index=False)      # 保存验证集，格式为text_a,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:37:00.020764Z",
     "iopub.status.busy": "2024-01-13T16:37:00.019314Z",
     "iopub.status.idle": "2024-01-13T16:37:02.947457Z",
     "shell.execute_reply": "2024-01-13T16:37:02.945858Z",
     "shell.execute_reply.started": "2024-01-13T16:37:00.020702Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: paddlenlp==2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (2.9.0)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (0.42.1)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (4.1.0)\r\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (2.2.0)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (1.2.2)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (0.70.11.1)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.1) (0.4.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.1) (1.16.0)\r\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.1) (1.20.3)\r\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp==2.0.1) (0.3.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.0.1) (0.24.2)\r\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (3.20.1)\r\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (8.2.0)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (1.0.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (2.22.0)\r\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (4.0.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (1.1.5)\r\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (1.21.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (2.2.3)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (0.8.53)\r\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (0.7.1.1)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.1) (1.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata<4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (4.2.0)\r\n",
      "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (2.8.0)\r\n",
      "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (2.4.0)\r\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (0.6.1)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.1) (3.0.0)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.1) (0.16.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.1) (7.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.1) (1.1.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.1) (2019.3)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.1) (2.8.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.1) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.1) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.1) (0.14.1)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.1) (0.18.0)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.1) (3.9.9)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.1) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.1) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.1) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.1) (2.8.2)\r\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (1.3.4)\r\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (16.7.9)\r\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (0.10.0)\r\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (1.3.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (5.1.2)\r\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (1.4.10)\r\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.1) (2.0.1)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.1) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.1) (1.25.6)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.1) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.1) (2019.9.11)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (4.7.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl->paddlenlp==2.0.1) (3.8.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp==2.0.1) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp==2.0.1) (56.2.0)\r\n",
      "\r\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.3.2\r\n",
      "[notice] To update, run: pip install --upgrade pip\r\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url\n",
    "\n",
    "!pip install paddlenlp==2.0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:37:04.236979Z",
     "iopub.status.busy": "2024-01-13T16:37:04.236347Z",
     "iopub.status.idle": "2024-01-13T16:37:04.242147Z",
     "shell.execute_reply": "2024-01-13T16:37:04.241281Z",
     "shell.execute_reply.started": "2024-01-13T16:37:04.236938Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:37:06.048152Z",
     "iopub.status.busy": "2024-01-13T16:37:06.047516Z",
     "iopub.status.idle": "2024-01-13T16:37:10.716996Z",
     "shell.execute_reply": "2024-01-13T16:37:10.715948Z",
     "shell.execute_reply.started": "2024-01-13T16:37:06.048109Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-14 00:37:06,050] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\r\n",
      "[2024-01-14 00:37:10,693] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"roberta-wwm-ext-large\"\n",
    "paddle.disable_static()\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14) \n",
    "# 此次分类任务为14分类任务，故num_classes设置为14\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# resnetmodel = paddle.vision.models.resnet50(pretrained=True, num_classes=12)\n",
    "# for param in resnetmodel.parameters():\n",
    "#     param.stop_gradient = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:38:19.613216Z",
     "iopub.status.busy": "2024-01-13T16:38:19.612044Z",
     "iopub.status.idle": "2024-01-13T16:38:21.041819Z",
     "shell.execute_reply": "2024-01-13T16:38:21.040608Z",
     "shell.execute_reply.started": "2024-01-13T16:38:19.613175Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\r\n",
      "<class '__main__.NewsData'>\r\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的14个类别\n",
    "label_list=list(train.label.unique())\n",
    "print(label_list)\n",
    "\n",
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',      # 验证集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签\n",
    "\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n",
    "\n",
    "# 加载训练和验证集\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])\n",
    "\n",
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# 参数设置：\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值  \n",
    "batch_size = 128\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。 通过文本长度分析可以看出文本长度最大为48，故此处设置为48\n",
    "max_seq_length = 48\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 验证集迭代器\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:38:22.964482Z",
     "iopub.status.busy": "2024-01-13T16:38:22.963074Z",
     "iopub.status.idle": "2024-01-13T16:38:22.972924Z",
     "shell.execute_reply": "2024-01-13T16:38:22.971961Z",
     "shell.execute_reply.started": "2024-01-13T16:38:22.964442Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 4\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:38:26.387707Z",
     "iopub.status.busy": "2024-01-13T16:38:26.386969Z",
     "iopub.status.idle": "2024-01-13T16:38:26.397738Z",
     "shell.execute_reply": "2024-01-13T16:38:26.396800Z",
     "shell.execute_reply.started": "2024-01-13T16:38:26.387666Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddle.fluid.core_avx.Generator at 0x7f801b09aeb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率\n",
    "\n",
    "# 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:52:40.359160Z",
     "iopub.status.busy": "2024-01-13T16:52:40.357701Z",
     "iopub.status.idle": "2024-01-13T17:27:36.265388Z",
     "shell.execute_reply": "2024-01-13T17:27:36.263942Z",
     "shell.execute_reply.started": "2024-01-13T16:52:40.359094Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.22057, acc: 0.95469\r\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.10519, acc: 0.95703\r\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.18701, acc: 0.95365\r\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.11724, acc: 0.95566\r\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.10822, acc: 0.95484\r\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.06769, acc: 0.95482\r\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.15951, acc: 0.95391\r\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.12835, acc: 0.95176\r\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.10898, acc: 0.95278\r\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.07153, acc: 0.95305\r\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.11862, acc: 0.95270\r\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.09202, acc: 0.95326\r\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.12978, acc: 0.95451\r\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.07931, acc: 0.95519\r\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.06772, acc: 0.95557\r\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.21993, acc: 0.95596\r\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.07199, acc: 0.95680\r\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.05788, acc: 0.95690\r\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.06751, acc: 0.95678\r\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.08421, acc: 0.95691\r\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.14069, acc: 0.95655\r\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.20594, acc: 0.95629\r\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.11543, acc: 0.95645\r\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.11890, acc: 0.95651\r\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.13503, acc: 0.95669\r\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.10698, acc: 0.95694\r\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.16295, acc: 0.95694\r\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.20105, acc: 0.95681\r\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.23180, acc: 0.95676\r\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.12433, acc: 0.95664\r\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.25624, acc: 0.95638\r\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.09659, acc: 0.95657\r\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.09283, acc: 0.95642\r\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.14098, acc: 0.95630\r\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.10279, acc: 0.95634\r\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.13829, acc: 0.95608\r\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.15949, acc: 0.95604\r\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.09465, acc: 0.95613\r\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.09845, acc: 0.95615\r\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.11587, acc: 0.95627\r\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.10934, acc: 0.95650\r\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.09128, acc: 0.95672\r\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.04799, acc: 0.95687\r\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.09514, acc: 0.95714\r\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.16608, acc: 0.95722\r\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.13025, acc: 0.95730\r\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.10705, acc: 0.95728\r\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.18628, acc: 0.95747\r\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.09222, acc: 0.95740\r\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.07682, acc: 0.95744\r\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.11953, acc: 0.95738\r\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.19202, acc: 0.95736\r\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.05812, acc: 0.95739\r\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.08813, acc: 0.95752\r\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.05615, acc: 0.95766\r\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.15516, acc: 0.95773\r\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.04604, acc: 0.95780\r\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.14201, acc: 0.95793\r\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.09822, acc: 0.95797\r\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.15602, acc: 0.95794\r\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.08208, acc: 0.95804\r\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.12416, acc: 0.95803\r\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.15340, acc: 0.95799\r\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.14659, acc: 0.95793\r\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.12157, acc: 0.95798\r\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.13532, acc: 0.95810\r\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.09047, acc: 0.95806\r\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.07552, acc: 0.95807\r\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.16106, acc: 0.95816\r\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.06986, acc: 0.95825\r\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.26269, acc: 0.95835\r\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.09778, acc: 0.95839\r\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.10847, acc: 0.95844\r\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.04200, acc: 0.95864\r\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.07842, acc: 0.95868\r\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.10492, acc: 0.95878\r\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.10162, acc: 0.95894\r\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.06287, acc: 0.95905\r\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.15188, acc: 0.95907\r\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.09185, acc: 0.95908\r\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.20437, acc: 0.95903\r\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.12660, acc: 0.95905\r\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.19035, acc: 0.95905\r\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.09827, acc: 0.95912\r\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.05377, acc: 0.95922\r\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.14095, acc: 0.95920\r\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.07974, acc: 0.95934\r\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.15233, acc: 0.95933\r\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.08934, acc: 0.95933\r\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.06549, acc: 0.95948\r\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.12271, acc: 0.95954\r\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.10078, acc: 0.95972\r\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.17275, acc: 0.95977\r\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.02655, acc: 0.95982\r\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.14195, acc: 0.95985\r\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.20142, acc: 0.95990\r\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.05155, acc: 0.95989\r\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.08048, acc: 0.96002\r\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.07707, acc: 0.96016\r\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.10150, acc: 0.96030\r\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.07423, acc: 0.96036\r\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.16900, acc: 0.96030\r\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.15398, acc: 0.96029\r\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.08999, acc: 0.96030\r\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.08142, acc: 0.96033\r\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.13517, acc: 0.96041\r\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.07749, acc: 0.96051\r\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.07199, acc: 0.96061\r\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.11544, acc: 0.96070\r\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.03908, acc: 0.96081\r\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.06599, acc: 0.96095\r\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.07608, acc: 0.96098\r\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.11057, acc: 0.96102\r\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.06698, acc: 0.96094\r\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.08822, acc: 0.96100\r\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.10697, acc: 0.96105\r\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.04099, acc: 0.96116\r\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.08924, acc: 0.96129\r\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.12671, acc: 0.96142\r\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.11407, acc: 0.96147\r\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.20026, acc: 0.96155\r\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.05070, acc: 0.96165\r\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.12307, acc: 0.96170\r\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.15762, acc: 0.96173\r\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.10234, acc: 0.96180\r\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.09862, acc: 0.96184\r\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.04350, acc: 0.96194\r\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.15207, acc: 0.96196\r\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.05972, acc: 0.96201\r\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.06697, acc: 0.96207\r\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.17388, acc: 0.96209\r\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.04726, acc: 0.96207\r\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.12388, acc: 0.96204\r\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.05114, acc: 0.96213\r\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.09599, acc: 0.96215\r\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.12332, acc: 0.96224\r\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.03701, acc: 0.96237\r\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.11606, acc: 0.96243\r\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.03375, acc: 0.96251\r\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.08510, acc: 0.96252\r\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.18667, acc: 0.96258\r\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.14488, acc: 0.96262\r\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.15705, acc: 0.96267\r\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.09524, acc: 0.96272\r\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.06391, acc: 0.96282\r\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.08248, acc: 0.96287\r\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.03297, acc: 0.96286\r\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.12733, acc: 0.96294\r\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.07021, acc: 0.96298\r\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.10097, acc: 0.96300\r\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.11343, acc: 0.96306\r\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.05361, acc: 0.96309\r\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.16056, acc: 0.96313\r\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.03072, acc: 0.96321\r\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.09435, acc: 0.96330\r\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.16025, acc: 0.96334\r\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.13712, acc: 0.96338\r\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.12234, acc: 0.96338\r\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.12432, acc: 0.96343\r\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.04274, acc: 0.96349\r\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.09953, acc: 0.96353\r\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.09197, acc: 0.96357\r\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.11752, acc: 0.96357\r\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.18884, acc: 0.96355\r\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.04444, acc: 0.96357\r\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.08664, acc: 0.96357\r\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.06474, acc: 0.96362\r\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.23262, acc: 0.96360\r\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.07121, acc: 0.96365\r\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.01606, acc: 0.96371\r\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.08084, acc: 0.96377\r\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.08027, acc: 0.96381\r\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.07137, acc: 0.96386\r\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.06494, acc: 0.96386\r\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.15079, acc: 0.96386\r\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.02555, acc: 0.96392\r\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.05760, acc: 0.96396\r\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.09382, acc: 0.96401\r\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.07210, acc: 0.96404\r\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.03765, acc: 0.96411\r\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.09624, acc: 0.96417\r\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.09735, acc: 0.96417\r\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.10806, acc: 0.96422\r\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.15762, acc: 0.96426\r\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.08652, acc: 0.96429\r\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.09124, acc: 0.96435\r\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.10902, acc: 0.96435\r\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.04010, acc: 0.96442\r\n",
      "global step 1890, epoch: 1, batch: 1890, loss: 0.14614, acc: 0.96442\r\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.08588, acc: 0.96449\r\n",
      "global step 1910, epoch: 1, batch: 1910, loss: 0.09366, acc: 0.96448\r\n",
      "global step 1920, epoch: 1, batch: 1920, loss: 0.15854, acc: 0.96455\r\n",
      "global step 1930, epoch: 1, batch: 1930, loss: 0.03202, acc: 0.96457\r\n",
      "global step 1940, epoch: 1, batch: 1940, loss: 0.05016, acc: 0.96461\r\n",
      "global step 1950, epoch: 1, batch: 1950, loss: 0.08531, acc: 0.96462\r\n",
      "global step 1960, epoch: 1, batch: 1960, loss: 0.08095, acc: 0.96469\r\n",
      "global step 1970, epoch: 1, batch: 1970, loss: 0.17973, acc: 0.96466\r\n",
      "global step 1980, epoch: 1, batch: 1980, loss: 0.07022, acc: 0.96470\r\n",
      "global step 1990, epoch: 1, batch: 1990, loss: 0.04627, acc: 0.96477\r\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.09972, acc: 0.96484\r\n",
      "global step 2010, epoch: 1, batch: 2010, loss: 0.09776, acc: 0.96487\r\n",
      "global step 2020, epoch: 1, batch: 2020, loss: 0.09087, acc: 0.96487\r\n",
      "global step 2030, epoch: 1, batch: 2030, loss: 0.04102, acc: 0.96494\r\n",
      "global step 2040, epoch: 1, batch: 2040, loss: 0.03321, acc: 0.96499\r\n",
      "global step 2050, epoch: 1, batch: 2050, loss: 0.04971, acc: 0.96505\r\n",
      "global step 2060, epoch: 1, batch: 2060, loss: 0.04534, acc: 0.96509\r\n",
      "global step 2070, epoch: 1, batch: 2070, loss: 0.09411, acc: 0.96511\r\n",
      "global step 2080, epoch: 1, batch: 2080, loss: 0.09421, acc: 0.96511\r\n",
      "global step 2090, epoch: 1, batch: 2090, loss: 0.06231, acc: 0.96512\r\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.06620, acc: 0.96517\r\n",
      "global step 2110, epoch: 1, batch: 2110, loss: 0.16025, acc: 0.96517\r\n",
      "global step 2120, epoch: 1, batch: 2120, loss: 0.04878, acc: 0.96522\r\n",
      "global step 2130, epoch: 1, batch: 2130, loss: 0.07323, acc: 0.96524\r\n",
      "global step 2140, epoch: 1, batch: 2140, loss: 0.06921, acc: 0.96524\r\n",
      "global step 2150, epoch: 1, batch: 2150, loss: 0.11115, acc: 0.96527\r\n",
      "global step 2160, epoch: 1, batch: 2160, loss: 0.06352, acc: 0.96527\r\n",
      "global step 2170, epoch: 1, batch: 2170, loss: 0.14627, acc: 0.96531\r\n",
      "global step 2180, epoch: 1, batch: 2180, loss: 0.08899, acc: 0.96533\r\n",
      "global step 2190, epoch: 1, batch: 2190, loss: 0.01676, acc: 0.96540\r\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.06505, acc: 0.96543\r\n",
      "global step 2210, epoch: 1, batch: 2210, loss: 0.18443, acc: 0.96544\r\n",
      "global step 2220, epoch: 1, batch: 2220, loss: 0.04122, acc: 0.96548\r\n",
      "global step 2230, epoch: 1, batch: 2230, loss: 0.05245, acc: 0.96550\r\n",
      "global step 2240, epoch: 1, batch: 2240, loss: 0.07579, acc: 0.96552\r\n",
      "global step 2250, epoch: 1, batch: 2250, loss: 0.10980, acc: 0.96552\r\n",
      "global step 2260, epoch: 1, batch: 2260, loss: 0.06479, acc: 0.96557\r\n",
      "global step 2270, epoch: 1, batch: 2270, loss: 0.03810, acc: 0.96560\r\n",
      "global step 2280, epoch: 1, batch: 2280, loss: 0.10337, acc: 0.96561\r\n",
      "global step 2290, epoch: 1, batch: 2290, loss: 0.08157, acc: 0.96559\r\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 0.09564, acc: 0.96559\r\n",
      "global step 2310, epoch: 1, batch: 2310, loss: 0.09245, acc: 0.96556\r\n",
      "global step 2320, epoch: 1, batch: 2320, loss: 0.03969, acc: 0.96558\r\n",
      "global step 2330, epoch: 1, batch: 2330, loss: 0.02798, acc: 0.96559\r\n",
      "global step 2340, epoch: 1, batch: 2340, loss: 0.06835, acc: 0.96561\r\n",
      "global step 2350, epoch: 1, batch: 2350, loss: 0.10143, acc: 0.96565\r\n",
      "global step 2360, epoch: 1, batch: 2360, loss: 0.05642, acc: 0.96568\r\n",
      "global step 2370, epoch: 1, batch: 2370, loss: 0.06209, acc: 0.96572\r\n",
      "global step 2380, epoch: 1, batch: 2380, loss: 0.02976, acc: 0.96576\r\n",
      "global step 2390, epoch: 1, batch: 2390, loss: 0.04945, acc: 0.96577\r\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_181/3921596986.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-314>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-312>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    226\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         optimize_ops = self._apply_optimize(\n\u001b[0;32m--> 366\u001b[0;31m             loss=None, startup_program=None, params_grads=params_grads)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_apply_optimize\u001b[0;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 params_grads = self.append_regularization_ops(\n\u001b[1;32m    795\u001b[0m                     params_grads, self.regularization)\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0moptimize_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_optimization_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mprogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/adamw.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[0;34m(self, parameters_and_grads)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_optimization_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         optimize_ops = super(\n\u001b[0;32m--> 203\u001b[0;31m             AdamW, self)._create_optimization_pass(parameters_and_grads)\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;31m# In dygraph mode, clear _lr_to_coeff after applied gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr_to_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[0;34m(self, parameters_and_grads)\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_optimize_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_and_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/adamw.py\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[0;34m(self, block, param_and_grad)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_append_optimize_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_decoupled_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_optimize_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/adamw.py\u001b[0m in \u001b[0;36m_append_decoupled_weight_decay\u001b[0;34m(self, block, param_and_grad)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_decay_param_fun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_decay_param_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_181/2300849668.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     apply_decay_param_fun=lambda x: x in [\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     ])\n",
      "\u001b[0;32m/tmp/ipykernel_181/2300849668.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     apply_decay_param_fun=lambda x: x in [\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"norm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     ])\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, include_sublayers)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             include_self=True) if include_sublayers else zip([prefix], [self])\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_sublayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_sublayers\u001b[0;34m(self, prefix, include_self, layers_set)\u001b[0m\n\u001b[1;32m    694\u001b[0m             for p, l in layer.named_sublayers(\n\u001b[1;32m    695\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                     layers_set=layers_set):\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_sublayers\u001b[0;34m(self, prefix, include_self, layers_set)\u001b[0m\n\u001b[1;32m    694\u001b[0m             for p, l in layer.named_sublayers(\n\u001b[1;32m    695\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                     layers_set=layers_set):\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_sublayers\u001b[0;34m(self, prefix, include_self, layers_set)\u001b[0m\n\u001b[1;32m    694\u001b[0m             for p, l in layer.named_sublayers(\n\u001b[1;32m    695\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                     layers_set=layers_set):\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_sublayers\u001b[0;34m(self, prefix, include_self, layers_set)\u001b[0m\n\u001b[1;32m    694\u001b[0m             for p, l in layer.named_sublayers(\n\u001b[1;32m    695\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                     layers_set=layers_set):\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36mnamed_sublayers\u001b[0;34m(self, prefix, include_self, layers_set)\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                     layers_set=layers_set):\n\u001b[0;32m--> 697\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "save_dir = \"checkpoint\"\n",
    "if not  os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    # 每轮结束对验证集进行评估\n",
    "    accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "    print(accu)\n",
    "    if accu > pre_accu:\n",
    "        # 保存较上一轮效果更优的模型参数\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        pre_accu=accu\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T16:38:29.978392Z",
     "iopub.status.busy": "2024-01-13T16:38:29.977630Z",
     "iopub.status.idle": "2024-01-13T16:40:43.908299Z",
     "shell.execute_reply": "2024-01-13T16:40:43.907312Z",
     "shell.execute_reply.started": "2024-01-13T16:38:29.978344Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from checkpoint/model_state.pdparams\r\n",
      "eval loss: 0.10180, accu: 0.96593\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965925"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载在验证集上效果最优的一轮的模型参数\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "    # 测试最优模型参数在验证集上的分数\n",
    "evaluate(model, criterion, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
