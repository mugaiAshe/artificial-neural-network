{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    " \n",
    "# from torchtext.data import Field,BucketIterator\n",
    "# from torchtext.datasets import Multi30k\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_seq=spacy.load(\"de_core_news_sm\")\n",
    "# en_seq=spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "# def de_tokenizer(text):\n",
    "#     return [word.text for word in de_seq.tokenizer(text)]\n",
    " \n",
    "# def en_tokenizer(text):\n",
    "#     return [word.text for word in en_seq.tokenizer(text)]\n",
    "\n",
    "# SRC=Field(tokenize=de_tokenizer,\n",
    "#          init_token=\"<sos>\",\n",
    "#          eos_token=\"<eos>\",\n",
    "#          lower=True,\n",
    "#          batch_first=True)\n",
    " \n",
    "# TRG=Field(tokenize=en_tokenizer,\n",
    "#          init_token=\"<sos>\",\n",
    "#          eos_token=\"<eos>\",\n",
    "#          lower=True,\n",
    "#          batch_first=True)\n",
    "\n",
    "# 自定义读取本地数据的方法\n",
    "def read(src_path, tgt_path, is_predict=False):\n",
    "    if is_predict:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f:\n",
    "            for src_line in src_f.readlines():\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':''}\n",
    "    else:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f, open(tgt_path, 'r', encoding='utf8') as tgt_f:\n",
    "            for src_line, tgt_line in zip(src_f.readlines(), tgt_f.readlines()):\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if not tgt_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':tgt_line}\n",
    " # 过滤掉长度 ≤min_len或者≥max_len 的数据            \n",
    "def min_max_filer(data, max_len, min_len=0):\n",
    "    # 1 for special tokens.\n",
    "    data_min_len = min(len(data[0]), len(data[1])) + 1\n",
    "    data_max_len = max(len(data[0]), len(data[1])) + 1\n",
    "    return (data_min_len >= min_len) and (data_max_len <= max_len)\n",
    "\n",
    "\n",
    "# 创建训练集、验证集的dataloader\n",
    "def create_data_loader(args):\n",
    "    train_dataset = load_dataset(read, src_path=args.training_file.split(',')[0], tgt_path=args.training_file.split(',')[1], lazy=False)\n",
    "    dev_dataset = load_dataset(read, src_path=args.validation_file.split(',')[0], tgt_path=args.validation_file.split(',')[1], lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    # 训练集dataloader和验证集dataloader\n",
    "    data_loaders = []\n",
    "    for i, dataset in enumerate([train_dataset, dev_dataset]):\n",
    "        dataset = dataset.map(convert_samples, lazy=False).filter(\n",
    "            partial(min_max_filer, max_len=args.max_length))\n",
    "\n",
    "        # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "        batch_sampler = BatchSampler(dataset,batch_size=args.batch_size, shuffle=True,drop_last=False)\n",
    "        \n",
    "        # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=partial(\n",
    "                prepare_train_input,\n",
    "                bos_idx=args.bos_idx,\n",
    "                eos_idx=args.eos_idx,\n",
    "                pad_idx=args.bos_idx),\n",
    "                num_workers=0,\n",
    "                return_list=True)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def prepare_train_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by training into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "    trg_word = word_pad([[bos_idx] + inst[1] for inst in insts])\n",
    "    lbl_word = np.expand_dims(\n",
    "        word_pad([inst[1] + [eos_idx] for inst in insts]), axis=2)\n",
    "\n",
    "    data_inputs = [src_word, trg_word, lbl_word]\n",
    "\n",
    "    return data_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试集的dataloader，原理步骤同上（创建训练集、验证集的dataloader）\n",
    "def create_infer_loader(args):\n",
    "    dataset = load_dataset(read, src_path=args.predict_file, tgt_path=None, is_predict=True, lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    dataset = dataset.map(convert_samples, lazy=False)\n",
    "\n",
    "    # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "    batch_sampler = BatchSampler(dataset,batch_size=args.infer_batch_size,drop_last=False)\n",
    "    \n",
    "    # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=partial(\n",
    "            prepare_infer_input,\n",
    "            bos_idx=args.bos_idx,\n",
    "            eos_idx=args.eos_idx,\n",
    "            pad_idx=args.bos_idx),\n",
    "            num_workers=0,\n",
    "            return_list=True)\n",
    "    return data_loader, trg_vocab.to_tokens\n",
    "\n",
    "def prepare_infer_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by beam search decoder into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "\n",
    "    return [src_word, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data,test_data=Multi30k.splits(exts=(\".de\",\".en\"),\n",
    "                                             fields=(SRC,TRG))\n",
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)\n",
    "\n",
    "batch=128\n",
    "device=torch.device(\"cpu\")#\"cuda\" if torch.cuda.is_available() else \n",
    " \n",
    "train_iter,val_iter,test_iter=BucketIterator.splits(\n",
    "    (train_data,val_data,test_data),\n",
    "    device=device,\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,src_vocab_size,emb_size,hid_size,kernel_size,n_layers,dropout=0.25,max_len=100):\n",
    "        #src_vocab_size 德语词库大小\n",
    "        #embe_size 嵌入维度\n",
    "        #hidden_size 卷积层的隐藏维度\n",
    "        #kernel_size 卷积核大小\n",
    "        #n_layers 卷积的block层数\n",
    "        super(Encoder,self).__init__()\n",
    "        self.token_emb=nn.Embedding(src_vocab_size,emb_size)\n",
    "        self.pos_emb=nn.Embedding(max_len,emb_size)\n",
    "        \n",
    "        self.emb2hid=nn.Linear(emb_size,hid_size)\n",
    "        self.hid2emb=nn.Linear(hid_size,emb_size)\n",
    "        \n",
    "        self.convs=nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=hid_size,\n",
    "                     out_channels=hid_size*2,\n",
    "                     kernel_size=kernel_size,\n",
    "                     padding=(kernel_size-1)//2)\n",
    "            for _  in range(n_layers)\n",
    "        ])\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)#其实是一个平均的过程\n",
    "        \n",
    "    def forward(self, src):\n",
    "        #src[batch src_len]\n",
    "        \n",
    "        #产生位置序列\n",
    "        batch_size=src.shape[0]\n",
    "        src_len=src.shape[1]\n",
    "        \n",
    "        pos=torch.arange(0,src_len).to(device)\n",
    "        #pos[src_len]\n",
    "        pos=pos.unsqueeze(0).repeat(batch_size,1)\n",
    "        #pos[batch src_len]\n",
    "        #src[batch src_len]\n",
    "        src_embed=self.token_emb(src)\n",
    "        pos_embed=self.pos_emb(pos)\n",
    "        #src[batch src_len emb_size]\n",
    "        #pos[batch src_len emb_size]\n",
    "        #词嵌入添加位置编码\n",
    "        src_pos_embed=self.dropout(src_embed+pos_embed)\n",
    "        \n",
    "        #src_pos_embed[batch src_len emb_size]\n",
    "        #转变维度使其进入卷积层\n",
    "        conv_input=self.emb2hid(src_pos_embed)\n",
    "        #conv_input[batch src_len hid_size]\n",
    "        \n",
    "        #注：1D卷积的输入shape为:[batch input_channel seq_len],input_channel为输入维度，\n",
    "        #           输出shape为:[batch output_channel  (seq_len+2*padding-kernel_size)/stride+1]\n",
    "        # 我们的padding=kernel_size-1//2(这样设计就是保住输入输出长度相同),stride=1，output_channel=2input_channel\n",
    "        # 因此输出:[batch 2*input_channel seq_len]\n",
    "        #所以首先先转变conv_input的shape\n",
    "        \n",
    "        conv_input=conv_input.permute(0,2,1)\n",
    "        #conv_input[batch hid_size src_len]\n",
    "        #进入卷积层\n",
    "        for conv in self.convs:\n",
    "            conved=conv(self.dropout(conv_input))\n",
    "            #conved[batch hid_size*2 src_len]\n",
    "            #输出为2*hid_size 是为了glu激活函数，其输出的维度是输入的一半\n",
    "            conved=F.glu(conved,dim=1)\n",
    "            #conved[batch hid_size src_len]\n",
    "            \n",
    "            #残差连接,防止网络退化 \n",
    "            conved=(conved+conv_input)*self.scale\n",
    "            #conved[batch hid_size src_len]\n",
    "            #循环遍历，此卷积输出是下一次卷积的输入\n",
    "            conv_input=conved\n",
    "            #conv_input[batch hid_size src_len]\n",
    "        \n",
    "        #卷积结束：\n",
    "        #conved[batch hid_size src_len]\n",
    "        \n",
    "        #转变shape\n",
    "        conved=conved.permute(0,2,1)\n",
    "        #conved[batch src_len hid_size]\n",
    "        \n",
    "        #转变维度，得到卷积向量,也是注意力机制的里面的k\n",
    "        conved=self.hid2emb(conved)\n",
    "        #conved[batch src_len emb_size]\n",
    "        \n",
    "        #残差连接，得到联合向量，也是注意力机制里面的v\n",
    "        combined=(conved+src_pos_embed)*self.scale\n",
    "        \n",
    "        #返回卷积向量和联合向量\n",
    "        return conved,combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size=len(SRC.vocab)\n",
    "trg_vocab_size=len(TRG.vocab)\n",
    " \n",
    "emb_size=256\n",
    "hid_size=512\n",
    "kernel_size=3\n",
    "n_layers=10\n",
    "\n",
    "enModel=Encoder(src_vocab_size,emb_size,hid_size,kernel_size,n_layers).to(device)\n",
    "conved,combined=enModel(src)\n",
    "print(conved.shape,combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,emb_size,hid_size):\n",
    "        #这里我默认了encoder与decoder的嵌入维度和隐层维度相同\n",
    "        super(Attention,self).__init__()\n",
    "        self.emb2hid=nn.Linear(emb_size,hid_size)\n",
    "        self.hid2emb=nn.Linear(hid_size,emb_size)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "    \n",
    "    def forward(self,dec_conved,embedd,en_conved,en_combined):\n",
    "        \"\"\"\n",
    "        注意力计算首先使用一个线性层改变Decoder传入的conved的隐藏维数为相同的嵌入维数。\n",
    "        然后，再与嵌入（embedded）通过一个残差连接求和。然后，通过发现它与编码的卷积（conved）有多少“匹配”，然后再通过对编码的组合（combined）进行加权和，\n",
    "        这样应用标准注意力计算。然后将其投影回隐藏的维度大小，并应用与注意力层初始输入（conved）的残差连接。\n",
    "        \"\"\"\n",
    "        #embedd[batch trg_len emb_size]\n",
    "        #dec_conved[batch hid_size trg_len]    Q（要加上词嵌入才算真正的Q）\n",
    "        #en_conved[batch src_len emb_size]     K \n",
    "        #en_combined[batch src_len emb_size]   V\n",
    "        \n",
    "        #转变Q的shape，使其为[batch trg_len hid_size]\n",
    "        dec_conved=dec_conved.permute(0,2,1)\n",
    "        #dec_conved[batch trg_len hid_size]\n",
    "        \n",
    "        #改变其维度，使其与嵌入维度相同\n",
    "        dec_conved_emb=self.hid2emb(dec_conved)\n",
    "        #dec_conved_emb[batch trg_len emb_size]\n",
    "        \n",
    "        #与embedded嵌入求和\n",
    "        Q=(dec_conved_emb+embedd)*self.scale\n",
    "        #Q[batch trg_len emb_size]\n",
    "        #en_conved[batch src_len emb_size]     K \n",
    "        \n",
    "        #计算与每个k的匹配程度\n",
    "        energy=torch.matmul(Q,en_conved.permute(0,2,1))\n",
    "        #energy[batch trg_len src_len]\n",
    "        a=F.softmax(energy,dim=2)\n",
    "        #a[batch trg_len src_len]\n",
    "        #en_combined[batch src_len emb_size]   V\n",
    "        \n",
    "        #得到权重以后计算其最终的向量\n",
    "        context=torch.matmul(a,en_combined)\n",
    "        #context[batch trg_len emb_size]\n",
    "        \n",
    "        #转变维度并加上卷积初始残差\n",
    "        #context[batch trg_len emb_size]\n",
    "        #dec_conved[batch trg_len hid_size]\n",
    "        context=self.emb2hid(context)\n",
    "        #context[batch trg_len hid_size]\n",
    "        conved=(context+dec_conved)*self.scale\n",
    "        #conved[batch trg_len hid_size]\n",
    "        return conved.permute(0,2,1),a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attModel=Attention(emb_size,hid_size).to(device)\n",
    "#自己造一个dec的卷积向量\n",
    "dec_conved=torch.randn(128,hid_size,26).to(device)\n",
    "#自己构造一个词嵌入（带有位置信息）--trg\n",
    "embedded=torch.randn(128,26,emb_size).to(device)\n",
    "dec_conved,a=attModel(dec_conved,embedded,conved,combined)\n",
    "print(dec_conved.shape,a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,trg_vocab_size,emb_size,hid_size,kernel_size,n_layers,attnModel,dropout=0.25,max_len=50):\n",
    "        #trg_vocab_size 英语的词库大小\n",
    "        #emb_size 嵌入维度\n",
    "        #hid_size 隐层维度\n",
    "        #kernel_size 卷积核大小\n",
    "        #n_layers 卷积网络的层数\n",
    "        #attnModel 注意力机制层\n",
    "        super(Decoder,self).__init__()\n",
    "        self.attnModel=attnModel\n",
    "        self.kernel_size=kernel_size#要根据其在前面创建kernel-1个pad\n",
    "        \n",
    "        self.token_embed=nn.Embedding(trg_vocab_size,emb_size)\n",
    "        self.pos_embed=nn.Embedding(max_len,emb_size)\n",
    "        \n",
    "        self.emb2hid=nn.Linear(emb_size,hid_size)\n",
    "        self.hid2emb=nn.Linear(hid_size,emb_size)\n",
    "        \n",
    "        self.fc=nn.Linear(emb_size,trg_vocab_size)\n",
    "        \n",
    "        self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.convs=nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=hid_size,\n",
    "                     out_channels=2*hid_size,\n",
    "                     kernel_size=kernel_size)\n",
    "            for _ in range(n_layers)])\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,trg,en_conved,en_combined):\n",
    "        #trg[batch trg_len]\n",
    "        #en_conved[batch src_len emb_size]\n",
    "        #en_combined[batch src_len emb_size]\n",
    "        \n",
    "        batch_size=trg.shape[0]\n",
    "        trg_len=trg.shape[1]\n",
    "        \n",
    "        #位置编码\n",
    "        pos=torch.arange(0,trg_len).to(device)\n",
    "        #pos[trg_len]\n",
    "        pos=pos.unsqueeze(0).repeat(batch_size,1)\n",
    "        #pos[batch trg_len]\n",
    "        \n",
    "        #嵌入并求和\n",
    "        token_embed=self.token_embed(trg)\n",
    "        pos_embed=self.pos_embed(pos)\n",
    "        #token_embed[batch trg_len emb_size]\n",
    "        #pos_embed[batch trg_len emb_size]\n",
    "        \n",
    "        embedd=self.dropout(token_embed+pos_embed)\n",
    "        #pos_embed[batch trg_len emb_size]\n",
    "        \n",
    "        #将embedd有emb_size维度转变为hid_size维度代入卷积层\n",
    "        input_conv=self.emb2hid(embedd).permute(0,2,1)\n",
    "        #input_conv[batch hid_size trg_len]\n",
    "        hid_size=input_conv.shape[1]\n",
    "        for _,conv in enumerate(self.convs):\n",
    "            input_conv=self.dropout(input_conv)\n",
    "            #对输入序列添加kernel_size的pad，防止翻译答案泄露\n",
    "            padding=torch.ones(batch_size,hid_size,self.kernel_size-1).to(device)\n",
    "            #padding[batch  hid_size kernel_size-1]\n",
    "            #input_conv[batch  hid_size trg_len]\n",
    "            pad_input_conv=torch.cat((padding,input_conv),dim=2)\n",
    "            #pad_input_conv[batch hid_size trg_len+kernel_size-1]\n",
    "            \n",
    "            conved=conv(pad_input_conv)\n",
    "            #conved[batch 2*hid_size trg_len]\n",
    "            conved=F.glu(conved,dim=1)\n",
    "            #conved[batch hid_size trg_len]\n",
    "            conved,a=self.attnModel(conved,embedd,en_conved,en_combined)\n",
    "            #conved[batch hid_size trg_len],a[batch trg_len src_len]\n",
    "            \n",
    "            #input_conv[batch hid_size trg_len]\n",
    "            #残差连接\n",
    "            conved=(conved+input_conv)*self.scale\n",
    "            #conved[batch  hid_size trg_len]\n",
    "            #带入下一层循环\n",
    "            input_conv=conved\n",
    "        \n",
    "        #卷积层出来后\n",
    "        #conved[batch  hid_size trg_len]\n",
    "        #转变维度为emb_size\n",
    "        output=self.hid2emb(conved.permute(0,2,1))\n",
    "        #output[batch trg_len emb_size]\n",
    "        #映射到英语字典空间上\n",
    "        output=self.fc(self.dropout(output))\n",
    "        return output,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "    \n",
    "    def forward(self,src,trg):\n",
    "        en_coved,en_combined=self.encoder(src)\n",
    "        output,attn=self.decoder(trg,en_coved,en_combined)\n",
    "        return output,attn\n",
    "\n",
    "model=Seq2Seq(enModel,deModel).to(device)\n",
    "output,a=model(src,trg)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,time\n",
    "from torch.optim import Adam\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "epochs=10\n",
    "clip=0.1\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=1)\n",
    "optim=Adam(model.parameters())\n",
    "\n",
    "def train(model,data_iter,criterion,optim,clip):\n",
    "    \n",
    "    model.train()\n",
    "    lossAll=0\n",
    "    for example in data_iter:\n",
    "        src=example.src\n",
    "        trg=example.trg\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        output,_=model(src,trg[:,:-1])\n",
    "        #output[batch trg_len-1 trg_vocab_size]\n",
    "        output=output.reshape(-1,trg_vocab_size)\n",
    "        trg=trg[:,1:].reshape(-1)\n",
    "        #output[batch*(trg_len-1),trg_vocab_size]\n",
    "        #trg[batch*(trg_ken-1)]\n",
    "        loss=criterion(output,trg)\n",
    "        loss.backward()      \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        optim.step()\n",
    "        \n",
    "        lossAll+=loss.item()\n",
    "    return lossAll/len(data_iter)\n",
    "\n",
    "def evaluate(model,data_iter,criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    lossAll=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for example in data_iter:\n",
    "            src=example.src\n",
    "            trg=example.trg\n",
    " \n",
    "            output,_=model(src,trg[:,:-1])\n",
    "            #output[batch trg_len-1 trg_vocab_size]\n",
    "            output=output.reshape(-1,trg_vocab_size)\n",
    "            trg=trg[:,1:].reshape(-1)\n",
    "            #output[batch*(trg_len-1),trg_vocab_size]\n",
    "            #trg[batch*(trg_ken-1)]\n",
    "            loss=criterion(output,trg)\n",
    "            lossAll+=loss.item()\n",
    "    return lossAll/len(data_iter)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
